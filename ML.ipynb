{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T13:35:47.748198Z",
     "start_time": "2025-12-17T13:34:29.572968Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ],
   "id": "f5d7149763b3626b",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T13:36:06.546295Z",
     "start_time": "2025-12-17T13:35:57.387190Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")"
   ],
   "id": "27780109f39c7903",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model architecture",
   "id": "2872c4e3c39afc97"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T13:36:10.251593Z",
     "start_time": "2025-12-17T13:36:09.898636Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(\"openai-community/gpt2\")\n",
    "print(config)"
   ],
   "id": "a11e6b1b39a205b4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.57.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T14:14:53.442616Z",
     "start_time": "2025-12-17T14:14:53.421673Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_text = \"Hello, I am a language model and I\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "print(inputs)\n",
    "# token=tokenizer.tokenize(input_text)\n",
    "# print(\"Tokens:\", token)\n",
    "# input_ids = tokenizer.convert_tokens_to_ids(token)\n",
    "# print(\"IDs List:\", input_ids)\n",
    "# import torch\n",
    "# input_ids = torch.tensor([input_ids])\n",
    "# print(\"Input IDs Tensor:\", input_ids)"
   ],
   "id": "f3731fc47d814a04",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[15496,    11,   314,   716,   257,  3303,  2746,   290,   314]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T14:17:45.893244Z",
     "start_time": "2025-12-17T14:17:19.807233Z"
    }
   },
   "cell_type": "code",
   "source": [
    "generated_ids = model.generate(\n",
    "    **inputs,\n",
    "    max_length=100,\n",
    "    num_beams=5,  # بررسی ۵ مسیر موازی برای یافتن بهترین جمله\n",
    "    no_repeat_ngram_size=2,  # جلوگیری از تکرار دو کلمه پشت سر هم\n",
    "    early_stopping=True,  # توقف به محض رسیدن به پایان منطقی\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "result = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "print(result)"
   ],
   "id": "acdaab313e559e50",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am a language model and I have a lot of work to do.\n",
      "\n",
      "I have been working on this project for a long time and it has been a pleasure to work with you. I would like to thank you for taking the time to answer my questions and to give me your feedback. If you have any questions or comments, please feel free to contact me.\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T14:19:34.398743Z",
     "start_time": "2025-12-17T14:19:28.555400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ذخیره مدل و توکنایزر در یک پوشه محلی\n",
    "model.save_pretrained(\"./my_gpt2_model\")\n",
    "tokenizer.save_pretrained(\"./my_gpt2_model\")"
   ],
   "id": "879af19ffbcf6b2e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./my_gpt2_model\\\\tokenizer_config.json',\n",
       " './my_gpt2_model\\\\special_tokens_map.json',\n",
       " './my_gpt2_model\\\\vocab.json',\n",
       " './my_gpt2_model\\\\merges.txt',\n",
       " './my_gpt2_model\\\\added_tokens.json',\n",
       " './my_gpt2_model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Fine tuning",
   "id": "11aafdf304bf128c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T14:34:50.571387Z",
     "start_time": "2025-12-17T14:34:45.758765Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# ۱. تنظیمات LoRA\n",
    "config = LoraConfig(\n",
    "    r=8,  # رتبه ماتریس‌ها (هر چه بیشتر باشد، ظرفیت یادگیری بیشتر و حافظه بیشتر مصرف می‌شود)\n",
    "    lora_alpha=32,  # ضریب مقیاس‌گذاری\n",
    "    target_modules=[\"c_attn\"],  # لایه‌های هدف در GPT-2 (معمولاً لایه‌های Attention)\n",
    "    lora_dropout=0.1,\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# ۲. اعمال تنظیمات روی مدل اصلی\n",
    "model = get_peft_model(model, config)\n",
    "\n",
    "# ۳. چاپ پارامترهای قابل آموزش\n",
    "model.print_trainable_parameters()"
   ],
   "id": "137f9d03e3c4135d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 294,912 || all params: 124,734,720 || trainable%: 0.2364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shahrooz\\PycharmProjects\\JupyterProject2\\.venv\\Lib\\site-packages\\peft\\tuners\\lora\\layer.py:2285: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T15:20:06.888041Z",
     "start_time": "2025-12-17T15:20:02.406077Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import TextDataset, DataCollatorForLanguageModeling\n",
    "\n",
    "# # ۱. آماده‌سازی دیتاست\n",
    "# def load_dataset(file_path, tokenizer):\n",
    "#     return TextDataset(\n",
    "#         tokenizer=tokenizer,\n",
    "#         file_path=file_path,\n",
    "#         block_size=128 # طول هر قطعه متن برای آموزش\n",
    "#     )\n",
    "#\n",
    "# train_dataset = load_dataset(\"my_data.txt\", tokenizer)\n",
    "#\n",
    "# ۲. جمع‌آوری‌کننده داده (Data Collator)\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ],
   "id": "70cc37206139cf48",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-12-17T15:20:09.247189Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# دانلود یک دیتاست آماده (مثلاً جملات کوتاه انگلیسی)\n",
    "train_dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train\")"
   ],
   "id": "341108a8d2001cf0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "10412492aa8d43458d43ca243fbfb27e"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "wikitext-2-raw-v1/test-00000-of-00001.pa(…):   0%|          | 0.00/733k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4ef247d14e8a4e2eb25b18ae6e315078"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "wikitext-2-raw-v1/train-00000-of-00001.p(…):   0%|          | 0.00/6.36M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0f589f3d4a2a40f1bd06785586c5e2b1"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "wikitext-2-raw-v1/validation-00000-of-00(…):   0%|          | 0.00/657k [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f46b6b27a96f4ede98ded9b4a9cbcdb9"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    },
    {
     "data": {
      "text/plain": [
       "Generating test split:   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a7cf9d97dec74569a85365be479a0db7"
      }
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"./gpt2-lora\",\n",
    "        per_device_train_batch_size=4,\n",
    "        learning_rate=3e-4,  # در LoRA معمولاً نرخ یادگیری را کمی بالاتر می‌برند\n",
    "        num_train_epochs=1,\n",
    "        logging_steps=10,\n",
    "    ),\n",
    "    train_dataset=train_dataset,  # همان دیتاستی که قبلاً ساختیم\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ],
   "id": "bb1085a5571f3c38"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "model.save_pretrained(\"./gpt2-lora-adapter\")",
   "id": "8ea222b7b2534271"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Inference",
   "id": "386dbed2e36e0b9f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "# الف. بارگذاری مدل اصلی (Base Model)\n",
    "base_model_name = \"openai-community/gpt2\"\n",
    "base_model = AutoModelForCausalLM.from_pretrained(base_model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "\n",
    "# ب. بارگذاری و سوار کردن آداپتور روی مدل اصلی\n",
    "model = PeftModel.from_pretrained(base_model, \"./gpt2-lora-adapter\")\n",
    "\n",
    "# حالا مدل آماده تولید متن با دانش جدید است\n",
    "inputs = tokenizer(\"Your prompt here\", return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_length=50)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ],
   "id": "4fa13edba668a28"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
